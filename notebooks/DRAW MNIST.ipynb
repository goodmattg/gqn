{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "from tensorflow.keras.layers import RNN, Flatten, RepeatVector\n",
    "from tensorflow.keras import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_ATTN = True\n",
    "WRITE_ATTN = True\n",
    "\n",
    "A, B = 28, 28  # image width,height\n",
    "img_size = B * A  # the canvas size\n",
    "enc_size = 256  # number of hidden units / output size in LSTM\n",
    "dec_size = 256\n",
    "\n",
    "# Glimpse grid dimensions ASSUMED TO BE ODD\n",
    "read_n = 5  # read glimpse grid width/height\n",
    "write_n = 5  # write glimpse grid width/height\n",
    "read_size = 2 * read_n * read_n if READ_ATTN else 2 * img_size\n",
    "write_size = write_n * write_n if WRITE_ATTN else img_size\n",
    "\n",
    "z_size = 10  # QSampler output size\n",
    "SEQ_LENGTH = 10  # MNIST generation sequence length\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_iters = 10000\n",
    "learning_rate = 1e-3  # learning rate for optimizer\n",
    "eps = 1e-8  # epsilon for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/mnist\"\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (3, 28, 28)\n",
      "Letter: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1354deda0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = train_ds.__iter__().next()\n",
    "sample = x[0]\n",
    "label = y[0]\n",
    "print(\"Batch Shape: {}\".format(x.shape))\n",
    "print(\"Letter: {}\".format(label))\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filterbank(gx, gy, sigma2, delta, N):\n",
    "    # Grid is 0-indexed, but formula is 1-indexed...\n",
    "    grid_i = tf.reshape(tf.cast(tf.range(1, N+1), tf.float32), [1, -1])\n",
    "    mu_x = gx + (grid_i - N / 2 - 0.5) * delta  # eq 19\n",
    "    mu_y = gy + (grid_i - N / 2 - 0.5) * delta  # eq 20\n",
    "    a_range = tf.reshape(tf.cast(tf.range(A), tf.float32), [1, 1, -1])\n",
    "    b_range = tf.reshape(tf.cast(tf.range(B), tf.float32), [1, 1, -1])\n",
    "    mu_x = tf.reshape(mu_x, [-1, N, 1])\n",
    "    mu_y = tf.reshape(mu_y, [-1, N, 1])\n",
    "    sigma2 = tf.reshape(sigma2, [-1, 1, 1])\n",
    "    Fx = tf.exp(-tf.square(a_range - mu_x) / (2 * sigma2))\n",
    "    Fy = tf.exp(-tf.square(b_range - mu_y) / (2 * sigma2))  # batch x N x B\n",
    "    \n",
    "    # normalize, sum over A and B dims\n",
    "    Fx = Fx / tf.maximum(tf.reduce_sum(Fx, [1,2], keepdims=True), eps)\n",
    "    Fy = Fy / tf.maximum(tf.reduce_sum(Fy, [1,2], keepdims=True), eps)\n",
    "    return Fx, Fy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRAWRNNCell(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 state_dim,\n",
    "                 read_attention=True,\n",
    "                 write_attention=True,\n",
    "                 read_n=5,\n",
    "                 write_n=5,\n",
    "                 z_size=10,\n",
    "                **kwargs):\n",
    "        \n",
    "        # Encoder/Decoder LSTM Cells track their own kernels\n",
    "        self.lstm_encoder = tf.keras.layers.LSTMCell(enc_size, kernel_initializer=\"zeros\")\n",
    "        self.lstm_decoder = tf.keras.layers.LSTMCell(dec_size, kernel_initializer=\"zeros\")\n",
    "               \n",
    "        self.enc_state = self.lstm_encoder.get_initial_state(\n",
    "            batch_size=BATCH_SIZE, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        self.dec_state = self.lstm_decoder.get_initial_state(\n",
    "            batch_size=BATCH_SIZE, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        self.use_read_attention = read_attention\n",
    "        self.use_write_attention = write_attention\n",
    "        \n",
    "        self.enc_size = enc_size\n",
    "        self.dec_size = dec_size\n",
    "        \n",
    "        self.output_size = output_dim\n",
    "        self.state_size = state_dim\n",
    "        \n",
    "        self.read_n = read_n\n",
    "        self.write_n = write_n\n",
    "        self.z_size = z_size\n",
    "        \n",
    "        self.q_sample_noise = tf.random.normal((BATCH_SIZE, z_size), mean=0, stddev=1)\n",
    "        \n",
    "        super(DRAWRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Attention Weights/Bias\n",
    "        self.attention_kernel = self.add_weight(\n",
    "            shape=(dec_size, self.read_n), initializer='glorot_uniform', name='W_att')\n",
    "        \n",
    "        self.attention_bias = self.add_weight(\n",
    "            shape=(self.read_n), initializer='zeros', name='b_att')\n",
    "        \n",
    "        # Q sample mu Weights/Bias\n",
    "        self.q_mu_kernel = self.add_weight(\n",
    "            shape=(dec_size, self.z_size),initializer='glorot_uniform',name='W_q_mu')\n",
    "        \n",
    "        self.q_mu_bias = self.add_weight(\n",
    "            shape=(self.z_size), initializer='zeros',name='b_q_mu')\n",
    "        \n",
    "        # Q sample sigma Weights/Bias\n",
    "        self.q_sigma_kernel = self.add_weight(\n",
    "            shape=(dec_size, self.z_size),initializer='glorot_uniform',name='W_q_sigma')\n",
    "        \n",
    "        self.q_sigma_bias = self.add_weight(\n",
    "            shape=(self.z_size), initializer='zeros',name='b_q_sigma')\n",
    "        \n",
    "        # Write Weights/Bias\n",
    "        if self.use_write_attention:\n",
    "            write_size = self.write_n * self.write_n\n",
    "        else:\n",
    "            # No attention, write the whole image\n",
    "            write_size = self.output_size\n",
    "        \n",
    "        self.write_kernel = self.add_weight(\n",
    "            shape=(dec_size, write_size), initializer='glorot_uniform', name='W_write')\n",
    "        \n",
    "        self.write_bias = self.add_weight(\n",
    "            shape=(write_size), initializer='zeros', name='b_write')\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def encode(self, input, state):\n",
    "        return self.lstm_encoder(input, state)\n",
    "    \n",
    "    def decode(self, input, state):\n",
    "        return self.lstm_decoder(input, state)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "\n",
    "        c_prev, h_dec_prev= states\n",
    "        x = inputs\n",
    "               \n",
    "        # Error image (3)\n",
    "        x_hat = x - tf.sigmoid(c_prev)\n",
    "        # Read (4)\n",
    "        glimpse = self.read(x, x_hat, h_dec_prev)\n",
    "        # Encode (5)\n",
    "        h_enc, self.enc_state = self.encode(tf.concat([glimpse, h_dec_prev], 1), self.enc_state)\n",
    "        # Sample (6)\n",
    "        z = self.sample_q(h_enc)\n",
    "        # Decode (7)\n",
    "        h_dec, self.dec_state = self.decode(z, self.dec_state)\n",
    "        # Write (8)\n",
    "        c_out = c_prev + self.write(h_dec)\n",
    "    \n",
    "        # Return canvas as output & pass along as part of state\n",
    "        return c_out, [c_out, h_dec]\n",
    "    \n",
    "    def read(self, x, x_hat, h_dec_prev):\n",
    "        if self.use_read_attention:\n",
    "            return self.read_attention(x, x_hat, h_dec_prev)\n",
    "        else:\n",
    "            return self.read_no_attention(x, x_hat, h_dec_prev)\n",
    "        \n",
    "    def read_no_attention(self, x, x_hat, h_dec_prev):\n",
    "        return tf.concat([x, x_hat], 1)\n",
    "\n",
    "    def read_attention(self, x, x_hat, h_dec_prev):\n",
    "        Fx, Fy, gamma = self.attn_window(h_dec_prev, read_n)\n",
    "\n",
    "        x = self._filter_batch(x, Fx, Fy, gamma, read_n)  # batch x (read_n*read_n)\n",
    "        x_hat = self._filter_batch(x_hat, Fx, Fy, gamma, read_n)\n",
    "        return tf.concat([x, x_hat], 1)  # concat along feature axis\n",
    "    \n",
    "    def write(self, h_dec):\n",
    "        if self.use_write_attention:\n",
    "            return self.write_attention(h_dec)\n",
    "        else:\n",
    "            return self.write_no_attention(h_dec)\n",
    "    \n",
    "    def write_no_attention(self, h_dec):\n",
    "        return tf.matmul(h_dec, self.write_kernel) + self.write_bias\n",
    "    \n",
    "    def write_attention(self, h_dec):\n",
    "        ww = tf.matmul(h_dec, self.write_kernel) + self.write_bias\n",
    "        N = self.write_n\n",
    "        ww = tf.reshape(ww, [BATCH_SIZE, N, N])\n",
    "        Fx, Fy, gamma = self.attn_window(h_dec, write_n)\n",
    "        Fyt = tf.transpose(Fy, perm=[0, 2, 1])\n",
    "        wr = tf.matmul(Fyt, tf.matmul(ww, Fx))\n",
    "        wr = tf.reshape(wr, [BATCH_SIZE, B * A])\n",
    "        return wr * tf.reshape(1.0 / gamma, [-1, 1])\n",
    "    \n",
    "    def attn_window(self, h_dec, N):\n",
    "        attention_params = tf.matmul(h_dec, self.attention_kernel) + self.attention_bias\n",
    "\n",
    "        gx_, gy_, log_sigma2, log_delta, log_gamma = tf.split(attention_params, 5, 1)\n",
    "        gx = (A + 1) / 2 * (gx_ + 1)\n",
    "        gy = (B + 1) / 2 * (gy_ + 1)\n",
    "        sigma2 = tf.exp(log_sigma2)\n",
    "        delta = (max(A, B) - 1) / (N - 1) * tf.exp(log_delta)  # batch x N\n",
    "        return gaussian_filterbank(gx, gy, sigma2, delta, N) + (tf.exp(log_gamma),)\n",
    "    \n",
    "    def _filter_batch(self, x, Fx, Fy, gamma, N):\n",
    "        x = tf.reshape(x, [-1, B, A])\n",
    "        Fxt = tf.transpose(Fx, perm=[0, 2, 1])\n",
    "        glimpse = tf.matmul(Fy, tf.matmul(x, Fxt))\n",
    "        glimpse = tf.reshape(glimpse, [-1, N * N])\n",
    "        return glimpse * gamma    \n",
    "   \n",
    "    def sample_q(self, h_enc):\n",
    "        mu = tf.matmul(h_enc, self.q_mu_kernel) + self.q_mu_bias\n",
    "        logsigma = tf.matmul(h_enc, self.q_sigma_kernel) + self.q_sigma_bias\n",
    "        sigma = tf.exp(logsigma)\n",
    "        return mu + sigma * self.q_sample_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = DRAWRNNCell(\n",
    "    img_size,\n",
    "    (img_size, dec_size),\n",
    "    read_attention=True,\n",
    "    write_attention=True)\n",
    "\n",
    "layer = RNN(cell)\n",
    "\n",
    "inputs = Input(batch_shape=(BATCH_SIZE, A, B))\n",
    "x = Flatten()(inputs)\n",
    "# Same input at each time step\n",
    "x = RepeatVector(SEQ_LENGTH)(x)\n",
    "y = layer(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x136ffdcf8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
