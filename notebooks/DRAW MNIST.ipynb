{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "from tensorflow.keras.layers import RNN, Flatten, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_ATTN = True\n",
    "WRITE_ATTN = True\n",
    "\n",
    "A, B = 28, 28  # image width,height\n",
    "img_size = B * A  # the canvas size\n",
    "enc_size = 256  # number of hidden units / output size in LSTM\n",
    "dec_size = 256\n",
    "\n",
    "# Glimpse grid dimensions ASSUMED TO BE ODD\n",
    "read_n = 5  # read glimpse grid width/height\n",
    "write_n = 5  # write glimpse grid width/height\n",
    "read_size = 2 * read_n * read_n if READ_ATTN else 2 * img_size\n",
    "write_size = write_n * write_n if WRITE_ATTN else img_size\n",
    "\n",
    "z_size = 10  # QSampler output size\n",
    "SEQ_LENGTH = 10  # MNIST generation sequence length\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_iters = 10000\n",
    "learning_rate = 1e-3  # learning rate for optimizer\n",
    "eps = 1e-8  # epsilon for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/mnist\"\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baby dataset\n",
    "micro_train_ds = train_ds.take(10)\n",
    "micro_test_ds = test_ds.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (3, 28, 28)\n",
      "Letter: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12cb0f9e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPXUlEQVR4nO3df5BV9XnH8c/DuqD8EAGFQcFoDY7SGgluoZROJXHqDzItapOONLU6psG02GhlprFmOqFJpuMUTZq0jQk2TNYkapwxKtOxKlIdJ9YQFkoFJAElqAgBFFNU6sLuPv1jj50N7vne5Z5z77nyvF8zO/fe89xzzzN397Pn3vs9537N3QXg2Des6gYANAdhB4Ig7EAQhB0IgrADQRzXzI0NtxF+vEY1c5NAKO/obR3ybhusVijsZnappK9JapP0r+5+W+r+x2uUZttFRTYJIGGNr86t1f0y3szaJP2LpMskTZe00Mym1/t4ABqryHv2WZJecPft7n5I0n2SFpTTFoCyFQn7aZJeGXB7Z7bsV5jZIjPrMrOuw+ousDkARRQJ+2AfArzn2Ft3X+7uHe7e0a4RBTYHoIgiYd8paeqA21Mk7SrWDoBGKRL2tZKmmdmZZjZc0lWSVpbTFoCy1T305u49ZnaDpMfUP/S2wt03l9YZgFIVGmd390ckPVJSLwAaiMNlgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiiqVM2o/kOXjE7Wfca/+7HPrMjWe/5xZ6j7AhVYc8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzt4Cho0cmazv+dPzk/WlSzpza5eMXJvedo3/9z/ptmT9s5uvStbt4Qm5tUlPvJpct2fHy8k6jk6hsJvZDklvSuqV1OPuHWU0BaB8ZezZP+Lur5XwOAAaiPfsQBBFw+6SHjezdWa2aLA7mNkiM+sys67D6i64OQD1Kvoyfq677zKziZJWmdlP3f3pgXdw9+WSlkvSiTbeC24PQJ0K7dndfVd2uVfSg5JmldEUgPLVHXYzG2VmY969LuliSZvKagxAuYq8jJ8k6UEze/dx7nH3R0vpKpi9V6fH0df87T8XePRiH8tMa//fZP0nM+9L1vtm5r9ze+Zv2pPrXvfop5P1c5ZuT9Z79+1L1qOpO+zuvl1S+q8UQMtg6A0IgrADQRB2IAjCDgRB2IEgOMW1CXo+ekGyfv1NDzds2+c/e02yPvKxMcn6pMd3Juu/uGRKsv7GeX25tT+78Knkuj+7/BvJ+paPHU7Wr73tr3Jrp3zz2eS6xyL27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQhLk378tjTrTxPtsuatr2mmZYW7Lct+rUZP2Rcx5K1g/6oWR9zreW5NamfrnGeHITf/9HGjZqVLL+0s3pkyqfvf6OZL1d+b+XCzf8SXLdCX/4UrLu3a35FWtrfLUO+P5Bv/+bPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4ewn2fWZOsl7rq6BrjaNffOvNyfpJd8c7N1uS3rgm/bz/4IvLcmtTjjshue7sdX+crE+88sVk3Xt6kvVGYZwdAGEHoiDsQBCEHQiCsANBEHYgCMIOBMH3xpfguN9/rdD6Mx77y2T97KDj6LWM60w/L5/Z8ue5tSX3pKeaXnPBPcn6vCsXJ+uj7/9xsl6Fmnt2M1thZnvNbNOAZePNbJWZbcsuxzW2TQBFDeVl/HckXXrEslskrXb3aZJWZ7cBtLCaYXf3pyXtP2LxAkmd2fVOSZeX3BeAktX7Ad0kd98tSdnlxLw7mtkiM+sys67Das3v7QIiaPin8e6+3N073L2jXSMavTkAOeoN+x4zmyxJ2eXe8loC0Aj1hn2lpHfnAr5GUuPmHAZQiprns5vZvZLmSTpZ0h5JX5D0kKT7JZ0u6WVJn3D3Iz/Ee4/38/nsL30x/9zp9dd9LbnulVuvSNb9ol3pjVf43e7HKrvg15P1ex+6K1lf2z02Wf/HOfOS9d59+5L1eqXOZ695UI27L8wpvT9TCwTF4bJAEIQdCIKwA0EQdiAIwg4EwSmumbc/PjtZX3PdV3Jr7TY8ue7WzVOS9Wn+arKO8vm6zcn6tp72ZP0jJ7yTrN/4F9OS9dP/rjFDbyns2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZM/uvejtZH5kYS/+nN9Jjqufenj6FtZrJfZGy+EufTdb/80vpabjtvANltlMK9uxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7Jnnf/t7yXqvD/rtvJKkbzx2cXLds15qvel7kTZx1SvJ+jOfT5/vvnHO3cn6fM086p6KYs8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzp7p9b5kfW13/rTJZ39zb/qx6+oIVfLRJyTrY4alvze+T21ltlOKmnt2M1thZnvNbNOAZUvN7FUz25D9zG9smwCKGsrL+O9IunSQ5V919xnZzyPltgWgbDXD7u5PS9rfhF4ANFCRD+huMLPnspf54/LuZGaLzKzLzLoOq7vA5gAUUW/Y75R0lqQZknZLuiPvju6+3N073L2jXSPq3ByAouoKu7vvcfded++TdJekWeW2BaBsdYXdzCYPuHmFpE159wXQGmqOs5vZvZLmSTrZzHZK+oKkeWY2Q5JL2iHp+gb22BI2vHN6bq132/YmdoJmeOVjpyTrHxqeHkc/6IfKbKcUNcPu7gsHWfztBvQCoIE4XBYIgrADQRB2IAjCDgRB2IEgOMW1BbSdPCFZ3/r1qcn6+LH5003bfScn1z3pu88m68eqtg+emaw/eeOyGo9wfLI658efTtanVnBoCnt2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfbMLXsuSNY/OS5/2uWV0+cm1+19fmuy/lrn+GR93fl3JutzvrUktzb1ezHH0SWp7aSxubXLHl6fXHfssPQ4ei1jHxhdaP1GYM8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzp558KfnJ+t/f2FXbm3LTfnjuZJ07udyZ8eSJJ06+kCyvr83PenzyF3500kPGzkyuW7f2/nnwrc6ax+erG9ZdnZu7eGT/qPQtg97+ncy/K30FOBVYM8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0GYe/4YbdlOtPE+2y5q2vaOxnFTTkvWn781v751Qfp8838/OCZZ/2Vveiz80dfPS9Y7z3git7bs9enJdR98OX18wZivn5istz+ef/xBLcPOPzdZ//mV6eMT5lyyMVlfPvWpo21pyM75weJk/YM353//QSOt8dU64PttsFrNPbuZTTWzJ81si5ltNrMbs+XjzWyVmW3LLtO/GQCVGsrL+B5JS9z9XEm/JWmxmU2XdIuk1e4+TdLq7DaAFlUz7O6+293XZ9fflLRF0mmSFkjqzO7WKenyRjUJoLij+oDOzM6Q9GFJayRNcvfdUv8/BEkTc9ZZZGZdZtZ1WN3FugVQtyGH3cxGS3pA0k3unj5zYwB3X+7uHe7e0a4R9fQIoARDCruZtas/6N939x9mi/eY2eSsPlnS3sa0CKAMNYfezMzU/558v7vfNGD5Mkmvu/ttZnaLpPHu/tepx2rlobeahrXllvrmfii56gufbE/WJ6zLf2xJmvRvP0/WP7Dyl7m12yc/nVy33dLbPuiH0vW+9KmeqeooS+9rRg9r3CvB/zqUPgV16az5yXrv6/vTG6jxvDRKauhtKOezz5V0taSNZrYhW3arpNsk3W9mn5L0sqRPlNEsgMaoGXZ3/5GkQf9TSHqf7qaBeDhcFgiCsANBEHYgCMIOBEHYgSA4xfUY133Zbybrf7BsdbK+8MTnkvWJbTW+qlqN+/v6n753kvVrX/x4bu3gl09Nrtv+xLq6eqpaoVNcARwbCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZkdQ7b2ay/s6E9Ln61sA/r/YDPen6+3SsvAjG2QEQdiAKwg4EQdiBIAg7EARhB4Ig7EAQQ/kqaQTW9tT6ZH1Uk/pAcezZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCImmE3s6lm9qSZbTGzzWZ2Y7Z8qZm9amYbsp/0hNYAKjWUg2p6JC1x9/VmNkbSOjNbldW+6u63N649AGUZyvzsuyXtzq6/aWZbJJ3W6MYAlOuo3rOb2RmSPixpTbboBjN7zsxWmNm4nHUWmVmXmXUdVnehZgHUb8hhN7PRkh6QdJO7H5B0p6SzJM1Q/57/jsHWc/fl7t7h7h3tGlFCywDqMaSwm1m7+oP+fXf/oSS5+x5373X3Pkl3SZrVuDYBFDWUT+NN0rclbXH3rwxYPnnA3a6QtKn89gCUZSifxs+VdLWkjWa2IVt2q6SFZjZDkkvaIen6hnQIoBRD+TT+R5IG+x7qR8pvB0CjcAQdEARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCHP35m3MbJ+klwYsOlnSa01r4Oi0am+t2pdEb/Uqs7cPuPspgxWaGvb3bNysy907KmsgoVV7a9W+JHqrV7N642U8EARhB4KoOuzLK95+Sqv21qp9SfRWr6b0Vul7dgDNU/WeHUCTEHYgiErCbmaXmtnPzOwFM7ulih7ymNkOM9uYTUPdVXEvK8xsr5ltGrBsvJmtMrNt2eWgc+xV1FtLTOOdmGa80ueu6unPm/6e3czaJG2V9HuSdkpaK2mhuz/f1EZymNkOSR3uXvkBGGb2u5LeknS3u/9GtuwfJO1399uyf5Tj3P1zLdLbUklvVT2NdzZb0eSB04xLulzStarwuUv09UdqwvNWxZ59lqQX3H27ux+SdJ+kBRX00fLc/WlJ+49YvEBSZ3a9U/1/LE2X01tLcPfd7r4+u/6mpHenGa/0uUv01RRVhP00Sa8MuL1TrTXfu0t63MzWmdmiqpsZxCR33y31//FImlhxP0eqOY13Mx0xzXjLPHf1TH9eVBVhH2wqqVYa/5vr7jMlXSZpcfZyFUMzpGm8m2WQacZbQr3TnxdVRdh3Spo64PYUSbsq6GNQ7r4ru9wr6UG13lTUe96dQTe73FtxP/+vlabxHmyacbXAc1fl9OdVhH2tpGlmdqaZDZd0laSVFfTxHmY2KvvgRGY2StLFar2pqFdKuia7fo2khyvs5Ve0yjTeedOMq+LnrvLpz9296T+S5qv/E/kXJX2+ih5y+vo1Sf+d/WyuujdJ96r/Zd1h9b8i+pSkCZJWS9qWXY5vod6+K2mjpOfUH6zJFfX2O+p/a/icpA3Zz/yqn7tEX0153jhcFgiCI+iAIAg7EARhB4Ig7EAQhB0IgrADQRB2IIj/A3HothjVediUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = train_ds.__iter__().next()\n",
    "sample = x[0]\n",
    "label = y[0]\n",
    "print(\"Batch Shape: {}\".format(x.shape))\n",
    "print(\"Letter: {}\".format(label))\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filterbank(gx, gy, sigma2, delta, N):\n",
    "    # Grid is 0-indexed, but formula is 1-indexed...\n",
    "    grid_i = tf.reshape(tf.cast(tf.range(1, N+1), tf.float32), [1, -1])\n",
    "    mu_x = gx + (grid_i - N / 2 - 0.5) * delta  # eq 19\n",
    "    mu_y = gy + (grid_i - N / 2 - 0.5) * delta  # eq 20\n",
    "    a_range = tf.reshape(tf.cast(tf.range(A), tf.float32), [1, 1, -1])\n",
    "    b_range = tf.reshape(tf.cast(tf.range(B), tf.float32), [1, 1, -1])\n",
    "    mu_x = tf.reshape(mu_x, [-1, N, 1])\n",
    "    mu_y = tf.reshape(mu_y, [-1, N, 1])\n",
    "    sigma2 = tf.reshape(sigma2, [-1, 1, 1])\n",
    "    Fx = tf.exp(-tf.square(a_range - mu_x) / (2 * sigma2))\n",
    "    Fy = tf.exp(-tf.square(b_range - mu_y) / (2 * sigma2))  # batch x N x B\n",
    "    \n",
    "    # normalize, sum over A and B dims\n",
    "    Fx = Fx / tf.maximum(tf.reduce_sum(Fx, [1,2], keepdims=True), eps)\n",
    "    Fy = Fy / tf.maximum(tf.reduce_sum(Fy, [1,2], keepdims=True), eps)\n",
    "    return Fx, Fy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRAWRNNCell(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 state_dim,\n",
    "                 read_attention=True,\n",
    "                 write_attention=True,\n",
    "                 read_n=5,\n",
    "                 write_n=5,\n",
    "                 z_size=10,\n",
    "                **kwargs):\n",
    "        \n",
    "        # Encoder/Decoder LSTM Cells track their own kernels\n",
    "        self.lstm_encoder = tf.keras.layers.LSTMCell(enc_size, kernel_initializer=\"zeros\")\n",
    "        self.lstm_decoder = tf.keras.layers.LSTMCell(dec_size, kernel_initializer=\"zeros\")\n",
    "               \n",
    "        self.enc_state = self.lstm_encoder.get_initial_state(\n",
    "            batch_size=BATCH_SIZE, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        self.dec_state = self.lstm_decoder.get_initial_state(\n",
    "            batch_size=BATCH_SIZE, dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        self.use_read_attention = read_attention\n",
    "        self.use_write_attention = write_attention\n",
    "        \n",
    "        self.enc_size = enc_size\n",
    "        self.dec_size = dec_size\n",
    "        \n",
    "        self.output_size = output_dim\n",
    "        self.state_size = state_dim\n",
    "        \n",
    "        self.read_n = read_n\n",
    "        self.write_n = write_n\n",
    "        self.z_size = z_size\n",
    "        \n",
    "        self.q_sample_noise = tf.random.normal((BATCH_SIZE, z_size), mean=0, stddev=1)\n",
    "        \n",
    "        super(DRAWRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # Attention Weights/Bias\n",
    "        self.attention_kernel = self.add_weight(\n",
    "            shape=(dec_size, self.read_n), initializer='glorot_uniform', name='W_att')\n",
    "        \n",
    "        self.attention_bias = self.add_weight(\n",
    "            shape=(self.read_n), initializer='zeros', name='b_att')\n",
    "        \n",
    "        # Q sample mu Weights/Bias\n",
    "        self.q_mu_kernel = self.add_weight(\n",
    "            shape=(dec_size, self.z_size),initializer='glorot_uniform',name='W_q_mu')\n",
    "        \n",
    "        self.q_mu_bias = self.add_weight(\n",
    "            shape=(self.z_size), initializer='zeros',name='b_q_mu')\n",
    "        \n",
    "        # Q sample sigma Weights/Bias\n",
    "        self.q_sigma_kernel = self.add_weight(\n",
    "            shape=(dec_size, self.z_size),initializer='glorot_uniform',name='W_q_sigma')\n",
    "        \n",
    "        self.q_sigma_bias = self.add_weight(\n",
    "            shape=(self.z_size), initializer='zeros',name='b_q_sigma')\n",
    "        \n",
    "        # Write Weights/Bias\n",
    "        if self.use_write_attention:\n",
    "            write_size = self.write_n * self.write_n\n",
    "        else:\n",
    "            # No attention, write the whole image\n",
    "            write_size = self.output_size\n",
    "        \n",
    "        self.write_kernel = self.add_weight(\n",
    "            shape=(dec_size, write_size), initializer='glorot_uniform', name='W_write')\n",
    "        \n",
    "        self.write_bias = self.add_weight(\n",
    "            shape=(write_size), initializer='zeros', name='b_write')\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def encode(self, input, state):\n",
    "        return self.lstm_encoder(input, state)\n",
    "    \n",
    "    def decode(self, input, state):\n",
    "        return self.lstm_decoder(input, state)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "\n",
    "        c_prev, h_dec_prev, _, _, _ = states\n",
    "        x = inputs\n",
    "               \n",
    "        # Error image (3)\n",
    "        x_hat = x - tf.sigmoid(c_prev)\n",
    "        # Read (4)\n",
    "        glimpse = self.read(x, x_hat, h_dec_prev)\n",
    "        # Encode (5)\n",
    "        h_enc, self.enc_state = self.encode(tf.concat([glimpse, h_dec_prev], 1), self.enc_state)\n",
    "        # Sample (6)\n",
    "        z, mu, log_sigma, sigma = self.sample_q(h_enc)\n",
    "\n",
    "        # Decode (7)\n",
    "        h_dec, self.dec_state = self.decode(z, self.dec_state)\n",
    "        # Write (8)\n",
    "        c_out = c_prev + self.write(h_dec)\n",
    "    \n",
    "        # Return canvas as output & pass along as part of state\n",
    "        return c_out, [c_out, h_dec, mu, log_sigma, sigma]\n",
    "    \n",
    "    def read(self, x, x_hat, h_dec_prev):\n",
    "        if self.use_read_attention:\n",
    "            return self.read_attention(x, x_hat, h_dec_prev)\n",
    "        else:\n",
    "            return self.read_no_attention(x, x_hat, h_dec_prev)\n",
    "        \n",
    "    def read_no_attention(self, x, x_hat, h_dec_prev):\n",
    "        return tf.concat([x, x_hat], 1)\n",
    "\n",
    "    def read_attention(self, x, x_hat, h_dec_prev):\n",
    "        Fx, Fy, gamma = self.attn_window(h_dec_prev, read_n)\n",
    "\n",
    "        x = self._filter_batch(x, Fx, Fy, gamma, read_n)  # batch x (read_n*read_n)\n",
    "        x_hat = self._filter_batch(x_hat, Fx, Fy, gamma, read_n)\n",
    "        return tf.concat([x, x_hat], 1)  # concat along feature axis\n",
    "    \n",
    "    def write(self, h_dec):\n",
    "        if self.use_write_attention:\n",
    "            return self.write_attention(h_dec)\n",
    "        else:\n",
    "            return self.write_no_attention(h_dec)\n",
    "    \n",
    "    def write_no_attention(self, h_dec):\n",
    "        return tf.matmul(h_dec, self.write_kernel) + self.write_bias\n",
    "    \n",
    "    def write_attention(self, h_dec):\n",
    "        ww = tf.matmul(h_dec, self.write_kernel) + self.write_bias\n",
    "        N = self.write_n\n",
    "        ww = tf.reshape(ww, [BATCH_SIZE, N, N])\n",
    "        Fx, Fy, gamma = self.attn_window(h_dec, write_n)\n",
    "        Fyt = tf.transpose(Fy, perm=[0, 2, 1])\n",
    "        wr = tf.matmul(Fyt, tf.matmul(ww, Fx))\n",
    "        wr = tf.reshape(wr, [BATCH_SIZE, B * A])\n",
    "        return wr * tf.reshape(1.0 / gamma, [-1, 1])\n",
    "    \n",
    "    def attn_window(self, h_dec, N):\n",
    "        attention_params = tf.matmul(h_dec, self.attention_kernel) + self.attention_bias\n",
    "\n",
    "        gx_, gy_, log_sigma2, log_delta, log_gamma = tf.split(attention_params, 5, 1)\n",
    "        gx = (A + 1) / 2 * (gx_ + 1)\n",
    "        gy = (B + 1) / 2 * (gy_ + 1)\n",
    "        sigma2 = tf.exp(log_sigma2)\n",
    "        delta = (max(A, B) - 1) / (N - 1) * tf.exp(log_delta)  # batch x N\n",
    "        return gaussian_filterbank(gx, gy, sigma2, delta, N) + (tf.exp(log_gamma),)\n",
    "    \n",
    "    def _filter_batch(self, x, Fx, Fy, gamma, N):\n",
    "        x = tf.reshape(x, [-1, B, A])\n",
    "        Fxt = tf.transpose(Fx, perm=[0, 2, 1])\n",
    "        glimpse = tf.matmul(Fy, tf.matmul(x, Fxt)) \n",
    "        glimpse = tf.reshape(glimpse, [-1, N * N])\n",
    "        return glimpse * gamma    \n",
    "   \n",
    "    def sample_q(self, h_enc):\n",
    "        mu = tf.matmul(h_enc, self.q_mu_kernel) + self.q_mu_bias\n",
    "        logsigma = tf.matmul(h_enc, self.q_sigma_kernel) + self.q_sigma_bias\n",
    "        sigma = tf.exp(logsigma)\n",
    "        return mu + sigma * self.q_sample_noise, mu, logsigma, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(t, o):\n",
    "    return -(t * tf.math.log(o + eps) + (1.0 - t) * tf.math.log(1.0 - o + eps))\n",
    "\n",
    "def joint_kl_loss(c_out, mu, log_sigma, sigma):\n",
    "    # Compute KL loss term\n",
    "    kl_terms = [0] * SEQ_LENGTH\n",
    "    \n",
    "    for t in range(SEQ_LENGTH):\n",
    "        mu2 = tf.square(mu[t])\n",
    "        sigma2 = tf.square(sigma[t])\n",
    "        logsig = log_sigma[t]\n",
    "        \n",
    "        kl_terms[t] = (\n",
    "            0.5 * tf.reduce_sum(mu2 + sigma2 - 2 * logsig, 1) - 0.5\n",
    "        )  # each kl term is (1xminibatch)\n",
    "    KL = tf.add_n(\n",
    "        kl_terms\n",
    "    )  # this is 1xminibatch, corresponding to summing kl_terms from 1:T\n",
    "    Lz = tf.reduce_mean(KL)  # average over minibatches\n",
    "        \n",
    "    def draw_loss(y_true, y_pred):\n",
    "        # reconstruction term appears to have been collapsed down to a single scalar value (rather than one per item in minibatch)\n",
    "        x = y_true\n",
    "        x_recons = y_pred\n",
    "\n",
    "        x_recons = tf.nn.sigmoid(c_out)\n",
    "\n",
    "        # after computing binary cross entropy, sum across features then take the mean of those sums across minibatches\n",
    "        Lx = tf.reduce_sum(binary_crossentropy(x, x_recons), 1)  # reconstruction term\n",
    "        Lx = tf.reduce_mean(Lx)\n",
    "\n",
    "        cost = Lx + Lz\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    return draw_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = DRAWRNNCell(\n",
    "    img_size,\n",
    "    (img_size, dec_size, SEQ_LENGTH, SEQ_LENGTH, SEQ_LENGTH),\n",
    "    read_attention=True,\n",
    "    write_attention=True)\n",
    "\n",
    "inputs = Input(batch_shape=(BATCH_SIZE, A, B))\n",
    "x = Flatten()(inputs)\n",
    "# Same input at each time step\n",
    "x = RepeatVector(SEQ_LENGTH)(x)\n",
    "rnn_out = RNN(cell, return_sequences=True, return_state=True)(x)\n",
    "canvas, _, _, mu, log_sigma, sigma = rnn_out\n",
    "\n",
    "optimizer = Adam(learning_rate, beta_1=0.5, clipnorm=5.0)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[canvas, mu, log_sigma, sigma])\n",
    "\n",
    "model.compile(\n",
    "    loss=joint_kl_loss(canvas, mu, log_sigma, sigma),\n",
    "    optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 4 array(s), but instead got the following list of 1 arrays: [<tf.Tensor: id=5752, shape=(3, 1), dtype=uint8, numpy=\narray([[0],\n       [2],\n       [3]], dtype=uint8)>]...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b405f0684155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    894\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m    895\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2448\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m   2451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    477\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    480\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 4 array(s), but instead got the following list of 1 arrays: [<tf.Tensor: id=5752, shape=(3, 1), dtype=uint8, numpy=\narray([[0],\n       [2],\n       [3]], dtype=uint8)>]..."
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, steps_per_epoch=10, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28), (None,)), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
